<?xml version="1.0" encoding="utf-8"?>
<section xml:id="nodes_sources"><info><title>Organizing your nodes</title></info>
		<para>
			The ProActive Resource Manager supports nodes aggregation from heterogeneous
			environments. As a node is just a JVM running somewhere, the process of communication
			to such nodes is unified and defined by ProActive library. The only part which has to
			be defined is the procedure of nodes deployment which could be quite different
			depending on infrastructures and their limitations.

			After <link linkend="rm-install">installation</link> of the server and node parts
			it is possible to configure an automatic nodes deployment.
			Basically, you can say to the resource manager how to launch JVMs with
			ProActive nodes and when.
		</para>

		<note><para>
			Lets give an example here. You have a cluster of linux-x64 machines
			permanently at your disposal with ssh access to all hosts
			and a cluster running Windows HPC where you would like
			to run computations periodically (let us say from 12 to 14 every day).
			In order to describe such a kind of behavior, we create 2 node sources
			using GUI or command line interface. The first one is the node source
			with "ssh infrastructure" and static deployment policy. The command would be
			the following:
			<screen><![CDATA[$RM_HOME>./bin/unix/rm-client --createns "MyCluster" -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.SSHInfrastructure
javaPath schedulingPath protocol port javaOptions hostsList]]></screen>
			The semantic of this command is quite straight forward, namely the resource
			manager tries immediately to launch the command of JVM creation through ssh using a special
			class which creates ProActive nodes inside JVMs and register them in the resource manager.
			Static node acquisition is default and you do not have to explicitly specify it.
		</para>
		<para>
			The procedure of deployment on Windows HPC is more tricky because it involves the native scheduler
			and in order to take into account other users of the cluster, we have to go through it.
			Basically, what the resource manager does is contact the windows scheduler
			exposed as a web service (it requires additional configuration of windows cluster)
			and submit a job launching the JVM process with ProActive. Here we also describe
			when the deployment has to be triggered using time slot policy.
			<screen><![CDATA[$RM_HOME>./bin/unix/rm-client --createns "WindowsCluster" -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.WinHPCInfrastructure
maxNodes serviceUrl userName password trustStore trustStorePassword javaPath rmPath RMUrl RMCredentialsPath javaOptions extraClassPath
-policy org.ow2.proactive.resourcemanager.nodesource.policy.TimeSlotPolicy nodesAvailableTo administrator acquireTime releaseTime period preemptive]]></screen>
		</para>
		</note>

		<para>
			As you can see in the example above, in order to create a node source, you have to define two entities
			<emphasis>infrastructure manager</emphasis> and
			<emphasis>node source policy</emphasis>.
		</para>
		<para>
			<emphasis>Infrastructure manager</emphasis> is responsible for communicating with an infrastructure.
			When a new node has to be deployed, an infrastructure manager will launch new JVM or just
			request an already existing nodes running somewhere. All these details are specific to the
			infrastructure manager implementation. ProActive library already has its own mechanism for
			deployment called <link xlink:href="http://proactive.inria.fr/trunk/Programming/ReferenceManual/multiple_html/GCMDeployment.html">
			ProActive Grid Component Model Deployment</link> and the resource manager reuses this framework
			adding another flexible mechanism to perform deployment.
		</para>

		<warning><para>
			ProActive Scheduling supports only one node per JVM. So that if you're configuring
			the resource manager to be used with scheduler take it into account.
			In the following, the term "node" always
			refers to a <emphasis>single node deployed in a dedicated JVM.</emphasis>
		</para></warning>

		<para>
			<emphasis>Node source policy</emphasis> is a set of rules and conditions which describes
			when and how many nodes have to be acquired or released. Policies use node source API to
			manage the node acquisition.
		</para>
		<para>
			Node sources were designed in a way that:
		</para>

		<itemizedlist>
			<listitem>
				<para>
					All logic related to node acquisition is encapsulated in the infrastructure manager.
				</para>
			</listitem>
			<listitem>
				<para>
					Conditions and rules of node acquisition is described in the node source policy.
				</para>
			</listitem>
			<listitem>
				<para>
					New infrastructure manager or node source policy can be dynamically plugged into the Resource Manager.
					In order to do that, it is just required to add new implemented classes in the class path and update
					corresponding list in the configuration file (<literal>[RM_HOME]/config/rm/nodesource</literal>).
				</para>
			</listitem>
		</itemizedlist>

		<para>
			In the resource manager, there is always a default node source consisted of DefaultInfrastrucureManager and
			Static policy. It is not able to deploy nodes anywhere but makes it possible to add existing nodes
			to the RM.
		</para>

		<section xml:id="default"><info><title>Default infrastructure</title></info>
			<para>
				<emphasis>Default infrastructure manager</emphasis> is designed to be used with ProActive agent.
				It cannot perform an automatic deployment but any users (including an agent) can add already existing nodes
				into it. In order to create a node source with this infrastructure, run the following command:
				<screen>$RM_HOME>./bin/unix/rm-client --createns defaultns -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.DefaultInfrastructureManager</screen>
			</para>
		</section>
		<section xml:id="RM_GCM"><info><title>GCM infrastructure</title></info>
			<para>
				<emphasis>GCM infrastructure manager</emphasis> is able to deploy nodes on the
				infrastructure described in GCM deployment descriptor.
				It supports only "deploy all" / "remove all" operations
				and cannot deploy a single node (due to GCMD limitations)
				and as a consequence cannot be used with some policies. Here is an example of such a node source
				creation:
			</para>
			<screen>$RM_HOME>./bin/unix/rm-client --createns gcmdns -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.GCMInfrastructure config/rm/deployment/Local4JVMDeployment.xml</screen>

			<para>The only argument that has to be provided to this infrastructure is the following one:</para>
			<itemizedlist>
				<listitem>
					<para><emphasis role="bold">descriptor</emphasis> - path to a GCM Deployment descriptor file.</para>
				</listitem>
			</itemizedlist>

			<para>
			    ProActive Resource Manager relies on GCM Deployment standard to deploy computing nodes.
				GCM standard implies that an application could be deployed anywhere without changing the source code.
				For instance, You can use various protocols such as rsh, ssh, Globus, LSF, etc. for the creation
				of the JVMs needed by the application. 
			    This section presents the main concepts of the GCM deployment mechanism, namely, how to write GCM Deployment descriptors
			    and how to use it with Resource Manager. If you want more details about the GCM Deployment, please refer to
				<link xlink:href="http://proactive.inria.fr/trunk/Programming/ReferenceManual/multiple_html/GCMDeployment.html">Chapter 14: ProActive grid Component Model Deployment</link>
				of the ProActive Programming manual.
			</para>

				<section xml:id="RM_GCM_concepts"><info><title>Main concepts</title></info>
					<section xml:id="RM_GCMA_GCMD"><info><title>GCM Application descriptor and GCM Deployment descriptor</title></info>
						<para>
							GCM Deployment procedure is described in two XML files:
								<itemizedlist>
									<listitem>
									<para>
										<emphasis>GCM Application Descriptor</emphasis> (GCMA) - defines the application to deploy. It
										 can be, for example, a native C/C++ application or a Java application. Any batch application can also be deployed with GCM.
										</para>
									</listitem>
									<listitem>
									<para>
										<emphasis> GCM Deployment Descriptor</emphasis> (GCMD) - describes where and how the application will be launched (deployed).
										This part defines host names that will run the application, the way (protocol) to access to these computers (ssh, rsh...),
										protocol-specific parameters to access to the host, file path to access to specified libraries regarding the host, etc.
										</para>
									</listitem>
								</itemizedlist>
						</para>
						<para>
						In the case of Resource Manager, GCMA always defines deployments of Proactive nodes. The only part an administrator has to define is
						the GCM deployment descriptor which contains a description of computers infrastructure.
						</para>
					</section>
					<section xml:id="RM_virtual_nodes"><info><title>Virtual Node</title></info>
						<para>
						GCM deployment and ProActve are built around the concept of virtual nodes which is
						an application level abstraction. At runtime, a virtual node will be mapped into one or several real nodes
						but an application does not really need to know that. The mapping is performed according to GCM deployment descriptor.
						</para>
					</section>
				</section>
				<section xml:id="RM_GCMA"><info><title>Use default GCM Application descriptor</title></info>
					<para>
					As said before, the application deployed by Resource Manager on remote computers is a ProActive node,
					i.e. a Java virtual machine with a ProActive Node (a ProActive node is a kind of container for ProActive applications),
					that will be provided to an application to be deployed on it or a scheduler to deploy tasks on it.
					So, a GCM Application descriptor is always the same and there is no need to change it. It is not mandatory to explore deeply the GCMA definition
					for using GCM deployment with ProActive Resource manager.
					The following GCMA is defined in the <emphasis>GCMNodeSourceApplication.xml</emphasis> file,
					stored in <emphasis>config/rm/deployment</emphasis> directory.
					</para>
					<programlisting language="xml"><textobject><textdata fileref="code_snippets/deployment/gcma.xml"/></textobject></programlisting>
					<para>
					Let us explain the different tags:
					</para>
					<itemizedlist>
						<listitem>
							<para><emphasis>environment tag</emphasis> - defines variables.
							This is a way to define variables that can be reused throughout the descriptor.
							Inside this tag, each variable can be reused (even in another following variable definition)
							by using the syntax ${name_of_variable}. The previous GCMA uses a <emphasis>JavaPropertyVariable</emphasis> variable named proactive.home.
							This kind of variable is therefore a Java Property variable, so not directly defined in the descriptor, but defined by
							the ProActive application that will use this file. The Java property proactive.home corresponds to the home directory of the ProActive application
							using this descriptor. Every ProActive application has this Java property variable defined.
							In this case, ProActive home directory corresponds to the home directory of Resource Manager Application
							(path in your file system where Resource Manager is stored). When Resource Manager performs parsing
							of this GCMApplication file, it replaces ${proactive.home} strings by Resource Manager home directory.
							If you want more information concerning definitions of variables in descriptors, see in ProActive Manual
							<link xlink:href="http://proactive.inria.fr/trunk/Programming/ReferenceManual/multiple_html/VariableContract.html">Chapter 18: Variable Contracts for Descriptors</link>.</para>
						</listitem>
						<listitem>
							<para><emphasis>application</emphasis> - the main tag in a GCMApplication descriptor, it describes the application to deploy.</para>
						</listitem>
						<listitem>
							<para><emphasis>proactive</emphasis> - the application to deploy is then a ProActive application. The attributes <emphasis>base</emphasis>
							and <emphasis>relpath</emphasis> define the ProActive home directory of the ProActive application that will be launched on a remote host, i.e.
							path in the file system to the remote host where ProActive is stored.
							Here, the path of application to launch is built from the root of the file system and the path corresponds
							to the environment variable ${proactive.home} defined above. So, ProActive home directory of the application to launch on remote hosts
							is the same as the Resource Manager path (this path can be overridden in GCMDeployment file).</para>
						</listitem>
						<listitem>
							<para><emphasis>configuration</emphasis> - defines configuration of ProActive application to launch. Here,
								the Java classpath of the JVM, that will be launched on a remote host, is defined. In the tag <emphasis>applicationClassPath</emphasis>,
								for each tag <emphasis>pathElement</emphasis> that defines the path to a jar file, this path will be added to the classpath.
								Thus, the paths to <emphasis role="italics">ProActive_Scheduler-worker.jar</emphasis> and <emphasis role="italics">ProActive_resourcemanager.jar</emphasis> will be added to the classpath of the JVM to deploy.
								These two paths are built from the <emphasis>base</emphasis> attribute set to 'proactive', that corresponds
								to the ProActive home directory of the deployed application in a remote host (not the ProActive home directory of the Resource Manager that performs
								the deployment), suffixed with the attribute <emphasis>relpath</emphasis> that points towards the <literal>dist/lib</literal> directory.
							</para>
							<note>
								<para>
									It is not necessary to add the <emphasis role="italics">ProActive_Scheduler-worker.jar</emphasis> file when you do not want to use the ProActive Scheduler on top of
									the Resource Manager. However, including this jar allows you to always have the same GCMA descriptor which is quite convenient.
								</para>
							</note>
						</listitem>
						<listitem><para><emphasis>virtualNode</emphasis> - this tag is used both by Resource Manager that performs deployment and deployed nodes.
						It defines a virtual node, with an id, that represents a set of nodes to deploy.
						For this virtual node, a <emphasis>nodeProvider</emphasis>, doing the association with a GCM Deployment descriptor, is defined. Basically,
						Resource Manager deploys all virtual nodes defined in this GCMA.</para>
						</listitem>
						<listitem><para><emphasis>resources</emphasis> - defines resources that must be acquired, i.e. defines association between a node Provider and a GCMDeployment file.
						You can see that the file path of the nodeProvider 'worker' does not look like a valid file path. The variable <emphasis>gcmd.file</emphasis>
						is automatically replaced by a path of a GCM Deployment descriptor that Administrator has asked to deploy. This line has not to be modified.</para>
						</listitem>
					</itemizedlist>
					<para>
					To sum up, this GCM application descriptor defines a ProActive application to deploy where there is one virtual node defined, called workers,
					and this virtual node will be mapped to real nodes, according to a GCM Deployment files that an administrator has defined and asked to deploy.
					</para>
				</section>
				<section xml:id="RM_GCMD"><info><title>Define GCM deployment descriptors</title></info>
					<para>
					Defining the GCM deployment files is the most important part of the Resource Manager administration since it is the way to aggregate
					all the computing resources of your infrastructure and to add them to Resource Manager. This part presents the deployment made
					by default, on the local machine. Then, you will see how to deploy nodes using <emphasis>ssh</emphasis> protocol.
					This chapter is not an exhaustive presentation of GCM Deployment descriptors.  It is recommended to refer to the
					<link xlink:href="http://proactive.inria.fr/trunk/Programming/ReferenceManual/multiple_html/GCMDeployment.html">Chapter 14: ProActive grid Component Model Deployment</link>
					of the ProActive manual.
					</para>
					<section xml:id="RM_GCMD_default"><info><title>Deployment of nodes on a local machine</title></info>
						<para>
						If you use the <literal>-ln</literal> options, Resource Manager deploys 4 ProActive nodes on the local machine. This deployment is performed by the GCM deployment descriptor
						<emphasis>Local4JVMDeployment[os].xml</emphasis>, stored in <emphasis>config/rm/deployment</emphasis> directory:
						</para>
						<programlisting language="xml"><textobject><textdata fileref="code_snippets/deployment/gcmdLocal.xml"/></textobject></programlisting>
						<para>
							<itemizedlist>
								<listitem><para><emphasis>environment</emphasis> - in the same way as in the GCMA, this tag defines variables that can be
								reused in the GCMD. A Java Property variable is defined, which is called 'user.home', that will be used later.</para>
								</listitem>
								<listitem><para><emphasis>resources</emphasis> - describes the hierarchical structure of the available grid resources,
								targets of deployment, and 'network route' to reach them.
								This can be seen as the topology of the grid: which hosts are part of a group? Which group is behind a bridge? ...
								Here, a host is defined directly in resource tag (without including it in a bridge or a group). Thus, this host
								is the local host.
								In this GCMD, one host is defined and it refers to the id <emphasis role="italics">'hLocalhost'</emphasis>. This reference Id points to a host definition in
								the infrastructure part.</para>
								</listitem>
								<listitem><para><emphasis>infrastructure</emphasis> - lists the grid resources on which the deployment can take place,
								in no particular order. Its purpose is to describe how these resources are deployed (i.e. through which protocols).
								In the case of a local deployment, only one resource is defined, i.e. one host. Inside the tag <emphasis>hosts</emphasis>,
								a <emphasis>host</emphasis> with an id (that is used by the resource part defined above) is inserted.
								For this host, <emphasis>os</emphasis> attribute defines its os type, <emphasis>hostCapacity</emphasis> attribute defines the number of
								JVMs that can be deployed on it and <emphasis>VMCapacity</emphasis> defines the number of ProActive nodes to be created per JVM.
								Here the host capacity is four and VM capacity is one, so we deploy 4 Java virtual machines, with one computing Node in each one.
								As for <emphasis>homeDirectory</emphasis>, it defines the path in remote host from where the program will be launched. In this case,
								the JVM  will be launched from the ${user.home} variable, that corresponds to resource Manager's user.home Java property.</para>
								</listitem>
							</itemizedlist>
						</para>
					</section>
					<section xml:id="RM_GCMD_ssh"><info><title>Deployment of nodes with ssh</title></info>
						<para>
						This section presents a way to deploy computing nodes on remote hosts, thanks to the ssh protocol. You can use this
						kind of deployment if you can perform connection through ssh between different hosts of your infrastructure.
						For using ssh with GCM, it is mandatory that your ssh connections do not need any password typing or key passphrase typing
						for authentication.
						</para>
						<para>
						You can use as a template, the file <emphasis>deployment_ssh_host_list.xml</emphasis> in the <emphasis>config/rm/deployment</emphasis> directory:
						</para>
						<programlisting language="xml"><textobject><textdata fileref="code_snippets/deployment/gcmd_ssh.xml"/></textobject></programlisting>
						<para>
							<itemizedlist>
							<listitem><para>
							First, a descriptor variable named <emphasis>HOSTS</emphasis> that contains a list of host names
							(targets of the deployment) is defined. Replace 'host1 host2 host3' by a list of hosts in your
							infrastructure that can be reached through ssh (separated by spaces).</para>
							</listitem>
							<listitem><para>
							In the <emphasis>resources</emphasis> tag, the group of machines reachable by ssh, that refers to ID
							'remoteThroughSSH', is defined. This group is more deeply described into the infrastructure tag. It contains hosts that have the
							ID 'host' (this kind of host is defined in infrastructure part too).</para>
							</listitem>
							<listitem><para>
							In the <emphasis>infrastructure</emphasis> tag, the host for the ID 'host' is defined.
							it is a unix host with a capacity of one (only one application can be run on it), and the JVM (effective application)
							of this node can handle one ProActive node. The application is started from the 'user.dir' descriptor environment variable.
							The 'remoteThroughSSH' group is then described.
							The <emphasis>sshGroup</emphasis> indicates that hosts contained in the host list <emphasis>${HOSTS}</emphasis> (variable defined into the <emphasis>environment</emphasis> tag) are reachable through the ssh protocol.
							</para>
							</listitem>
							</itemizedlist>
						</para>
					</section>
					<section xml:id="RM_launch_with_gcmd"><info><title>Launch Resource Manager with your specific deployment descriptor</title></info>
						<para>
						To start the Resource Manager with your own GCM deployment descriptor, run the <emphasis>rm-start[.bat]</emphasis>
						script in <literal>'bin/[os]/'</literal> directory and put in paramter the path of your GCM Deployment file. If you want to deploy Resource Manager
						with the <literal>deployment_ssh_host_list.xml</literal> file defined above, type in the <literal>'bin/[os]/'</literal>1 directory:
						<screen>rm-start -d ../../config/rm/deployment/deployment_ssh_host_list.xml</screen>
						</para>
						<para>
						Resource Manager starts and deploys computing nodes on hosts defined in the GCM deployment file. You can put in parameters of this
						command several deployment descriptors. Resource Manager automatically associates GCMD files in parameters with the GCMA
						<emphasis>GCMNodeSourceApplication.xml</emphasis>, stored in <emphasis>config/rm/deployment</emphasis> directory.
						</para>
					</section>
					<section xml:id="RM_launch_with_gcmd_custom_path"><info><title>Customize in GCMD resource Manager Path and Java path of your
					remote hosts</title></info>
						<para>
						In the previous example, we assumed that Java command on remote hosts is automatically reachable (i.e. is in the $PATH),
						and that ProActive libraries on remote hosts are stored in the same path in file system than for Resource Manager. However, it is not always the case.
						To face this problem, you can define these
						paths specifically for each computing resource. These paths can be different from resource manager, and even be different between hosts
						on which you want to perform the deployment.
						</para>
						<para>
						To illustrate this aspect, a new GCM Deployment file that deploys two computing nodes on hosts named
						'computer1' and 'computer2', where Java executable is not installed in the same paths for the two hosts, is defined. Look at the
						<emphasis>deployment_ssh.xml</emphasis> descriptor in the <emphasis>config/rm/deployment</emphasis> directory:
						</para>
						<programlisting language="xml"><textobject><textdata fileref="code_snippets/deployment/gcmd_custom.xml"/></textobject></programlisting>
					</section>
				</section>
		</section>
		<section xml:id="GCM_customized"><info><title>GCM customized infrastructure</title></info>
				<para>
					<emphasis>GCM customized infrastructure</emphasis> can deploy/remove
					a single node to/from the infrastructure described in GCM descriptor.
					It makes possible to use this insfrastructure manager together with
					more precise policies such as "scheduler aware policy" (see below).
					In order to create such node source user has to specify GCMD
					and hosts list. GCMD has to define a deployment of
					a single node and does not have to have an explicit host names.
					Instead of this, ${HOST} variable must be used and at the
					moment of deployment the resource manager associates host
					to the gcmd (see <literal>$RM_HOME/config/rm/deployment/deployment_ssh_hosts_list_template.xml</literal>).
				</para>
				<screen>$RM_HOME>./bin/unix/rm-client --createns gcmns -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.GCMCustomisedInfrastructure config/rm/deployment/deployment_ssh_hosts_list_template.xml config/rm/deployment/hostslist</screen>
				<para>This infrastructure needs 2 arguments, described hereafter:</para>
				<itemizedlist>
					<listitem>
						<para><emphasis role="bold">descriptor</emphasis> - path to a template GCM Deployment descriptor file where the <literal>${HOST}</literal> variable is used.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">hostsList</emphasis> - path to a file containing a list of hosts (one host per line).</para>
					</listitem>
				</itemizedlist>
		</section>
		<section xml:id="SSH"><info><title>SSH infrastructure</title></info>
			<para>
				This infrastructure allows to deploy nodes over ssh. Having a list of hosts the resource manager
				construct the ssh command to launch remote JVMs. Java path and rm distribution path on remote hosts
				have to be specified together with other parameters. Example:
			</para>
			<screen>$RM_HOME>./bin/unix/rm-client --createns sshns -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.SSHInfrastructure JAVA_PATH RM_HOME rmi 1099 \"\" config/rm/deployment/hostslist</screen>
			<para>This infrastructure needs 6 arguments, described hereafter:</para>
			<itemizedlist>
				<listitem>
					<para>
						<emphasis role="bold">Java Path</emphasis> - Path to the java executable on the remote hosts.
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis role="bold">Scheduling Path</emphasis> - Path to the Scheduling/RM installation directory on the remote hosts.
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis role="bold">Protocol</emphasis> - Protocol (rmi, http...) used by the remote host to communicate with the RM.
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis role="bold">Port</emphasis> - Port used by the remote host to communicate with the RM.
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis role="bold">Java Options</emphasis> - Java options appended to the command used to start the node on the remote host.
					</para>
				</listitem>
				<listitem>
					<para>
						<emphasis role="bold">Hosts List</emphasis> - Path to a file containing the hosts on which resources should be acquired.
						This file should contain one host per line, described as a host name or a public IP address, optionally followed by a positive
						integer describing the number of runtimes to start on the related host (default to 1 if not specified). Example:
						<screen><![CDATA[rm.example.com
test.example.net 5
192.168.9.10 2]]></screen>
					</para>
				</listitem>
			</itemizedlist>
		</section>
		<section xml:id="Win_HPC"><info><title>Windows HPC infrastructure</title></info>
				<para>
					The deployment through Windows HPC scheduler. In order to make it functional,
					the Windows HPC has to be configured according to
					<link xlink:href="http://technet.microsoft.com/en-us/library/cc972837(WS.10).aspx">this guide</link>.
					After exposing windows native scheduler as a web service it is possible to contact
					it from Java and submit any command. In our case, it is a command launching JVM on one of the
					cluster nodes.
				</para>
				<screen>$RM_HOME>./bin/unix/rm-client --createns winhpc -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.WinHPCInfrastructure 8 https://cluster_name/HPCBasicProfile username password trustStore trustStorePassword javaPath rmPath RMUrl RMCredentialsPath javaOptions extraClassPath</screen>

				<para>This infrastructure needs 12 arguments, described hereafter:</para>
				<itemizedlist>
					<listitem>
						<para><emphasis role="bold">maxNodes</emphasis> - Maximum number of nodes to deploy.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">serviceUrl</emphasis> - Url of the WinHPC web service.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">userName</emphasis> - username for the web service connection</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">password</emphasis> - password for the web service connection</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">trustStore</emphasis> - name of the trustStore</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">trustStorePassword</emphasis> - password of the trustStore</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">javaPath</emphasis> - Path to the java executable on the WinHPC server.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">rmPath</emphasis> - Path to the Resource Manager directory on the WinHPC server.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">RMUrl</emphasis> - Url of the resource manager server</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">RMCredentialsPath</emphasis> - Path to the RM Credentials</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">javaOptions</emphasis> - Java options appended to the command used to start the node on WinHPC server.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">extraClassPath</emphasis> - Extra class path to be added.</para>
					</listitem>
				</itemizedlist>
		</section>

		<!-- AMAZON EC2 section -->
		&EC2;

		<section xml:id="PBS"><info><title>Portable Batch System (PBS) infrastructure</title></info>
				<para>
				This infrastructure knows how to acquire nodes from PBS (i.e. Torque) by submitting
				a corresponding job. It will be submitted through SSH from the RM to the PBS server.
				As an alternative, you may consider using <link linkend="RM_GCM">the GCM infrastructure</link>
				instead, which provides more control over the deployment, but requires more configuration.
				</para>
				<screen>$RM_HOME>./bin/unix/rm-client --createns pbs -infrastructure org.ow2.proactive.resourcemanager.nodesource.infrastructure.PBSInfrastructure javaPath schedulingPath protocol port javaOptions maxNodes PBSServer RMUrl RMCredentialsPath qsubOptions</screen>
				<para>where:</para>
			    <itemizedlist>
			      <listitem><para>
				  <emphasis role="bold">javaPath</emphasis> - path to the java executable on the remote hosts (ie the PBS slaves).
			      </para></listitem>
			      <listitem><para>
				  <emphasis role="bold">schedulingPath</emphasis> - path to the Scheduling/RM installation directory on the remote hosts.
			      </para></listitem>
			      <listitem><para>
				  <emphasis role="bold">protocol</emphasis> - protocol (rmi, http...) used by the remote host to communicate with the RM.
			      </para></listitem>
			      <listitem><para>
				  <emphasis role="bold">port</emphasis> - port used by the remote host to communicate with the RM.
			      </para></listitem>
			      <listitem><para>
				  <emphasis role="bold">javaOptions</emphasis> - Java options appended to the command used to start the node
				  on the remote host.
			      </para></listitem>
			      <listitem><para>
				  <emphasis role="bold">maxNodes</emphasis> - maximum number of nodes this infrastructure can simultaneously hold from the PBS server.
				  That is useful considering that PBS does not provide a mechanism to evaluate the number of currently available or idle cores on the cluster.
				  This can result to asking more resources than physically available, and waiting for the resources to come up for a very long time
				  as the request would be queued until satisfiable.
			      </para></listitem>
			      <listitem><para>
				  <emphasis role="bold">PBSServer</emphasis> - URL of the PBS server, which is responsible for acquiring PBS nodes.
				  This server will be contacted by the Resource Manager through an SSH connection.
			      </para></listitem>
			      <listitem><para>
				  <emphasis role="bold">RMUrl</emphasis> - URL of the Resource Manager from the PBS nodes point of view -
				  this is the URL the nodes will try to lookup when attempting to register to the RM after their creation.
			      </para></listitem>
			      <listitem><para>
				  <emphasis role="bold">RMCredentials</emphasis> - Encrypted credentials file, as created by the create-cred[.bat] utility.
				  These credentials will be used by the nodes to authenticate on the Resource Manager.
			      </para></listitem>
			      <listitem><para>
				  <emphasis role="bold">qsubOptions</emphasis> - Options for the qsub command client when acquiring nodes on the PBS master.
				  Default value should be enough in most cases, if not, refer to the documentation of the PBS cluster.
			      </para></listitem>
			    </itemizedlist>
		</section>
		&Virtualization;
		<section xml:id="static"><info><title>Static policy</title></info>
				<para>
					<emphasis>Static node source policy</emphasis> starts node acquisition when nodes are added to
					the node source and never removes them. Nevertheless, nodes can be removed by user request.
				</para>
				<para>For using this policy, you have to precise the following parameters:</para>
				<itemizedlist>
					<listitem>
						<para><emphasis role="bold">nodesAvailableTo</emphasis> - utilization permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator </para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">administrator</emphasis> - Administration permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator </para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
				</itemizedlist>
		</section>
		<section xml:id="time"><info><title>Time slot policy</title></info>
				<para>
					<emphasis>Time slot policy</emphasis> is aimed to acquire nodes for particular time with an ability to do it periodically.
				</para>
				<para>For using this policy, you have to precise the following parameters:</para>
				<itemizedlist>
					<listitem>
						<para><emphasis role="bold">nodesAvailableTo</emphasis> - utilization permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">administrator</emphasis> - Administration permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">acquireTime</emphasis> - Absolute acquire date (e.g. "6/3/10 1:18:45 PM CEST").</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">releaseTime</emphasis> - Absolute releasing date (e.g. "6/3/10 2:18:45 PM CEST").</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">period</emphasis> - period time in millisecond (default is 86400000).</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">preemptive</emphasis> - Preemptive parameter indicates the way of releasing nodes. If it is true, nodes will be released
						without waiting the end of jobs running on (default is false).</para>
					</listitem>
				</itemizedlist>

		</section>
		<section xml:id="scheduler_idle"><info><title>"Remove nodes when scheduler is idle" policy</title></info>
				<para>
					<emphasis>"Remove nodes when scheduler is idle" policy</emphasis> removes all nodes from the infrastructure
					when the scheduler is idle and acquires them when a new job is submitted. This policy may be useful 
					if there is no need to keep nodes alive permanently. Nodes will be released after a specified "idle time". 
					This policy will use a listener of the scheduler, that is why its URL, its user name and its password have to be specified.
				</para>
				<para>For using this policy, you have to precise the following parameters:</para>
				<itemizedlist>
					<listitem>
						<para><emphasis role="bold">nodesAvailableTo</emphasis> - utilization permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">administrator</emphasis> - Administration permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">schedulerUrl</emphasis> - Url of the Scheduler</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">schedulerCredentialsPath</emphasis> - Path to the credentials used for scheduler authentification.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">idleTime</emphasis> - idle time in millisecond to wait before removing all nodes (default is 60000).</para>
					</listitem>
				</itemizedlist>
				<note>
					<para>
						This policy is available only when using the <emphasis>ProActive Scheduler</emphasis>.
					</para>
				</note>
		</section>
		<section xml:id="scheduler_dynamic"><info><title>"Scheduler loading" policy</title></info>
				<para>
					<emphasis>"Scheduler loading" policy</emphasis> acquires/releases nodes
					according to the scheduler loading factor. This policy allows to configure the number of resources 
					which will be always enough for the scheduler. Nodes are acquired and released according to scheduler loading factor 
					which is a number of tasks per node. In the same manner as the previous policy, this one also requires scheduler URL, 
					user name and password. It is important to correctly configure maximum and minimum nodes that this policy will try to hold. 
					Maximum number should not be greater than potential nodes number which is possible to deploy to underlying infrastructure. 
					If there are more currently acquired nodes than necessary, policy will release them one by one after having waited for a "release period" delay. 
					This smooth release procedure is implemented because deployment time is greater than the release one. 
					Thus, this waiting time deters policy from spending all its time trying to deploy nodes.
				</para>
				<para>For using this policy, you have to precise the following parameters:</para>
				<itemizedlist>
					<listitem>
						<para><emphasis role="bold">nodesAvailableTo</emphasis> - utilization permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">administrator</emphasis> - Administration permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">schedulerUrl</emphasis> - Url of the Scheduler</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">schedulerCredentialsPath</emphasis> - Path to the credentials used for scheduler authentification.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">refreshTime</emphasis> - time between each calculation of the number of needed nodes.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">minNodes</emphasis> - Minimum number of nodes to deploy</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">maxNodes</emphasis> - Maximum number of nodes to deploy</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">loadFactor</emphasis> - number of tasks per node. Actually, if this number is <emphasis role="italics">N</emphasis>,
						it does not means that there will be exactly <emphasis role="italics">N</emphasis> tasks
						executed on each node. This factor is just used to compute the total number of nodes. For instance, let us assume that this factor is 3 and that we schedule 100
						tasks. In that case, we will have 34 (= upper bound of 100/3) started nodes. Once one task finished and the refresh time passed,
						one node will be removed since 99 divided by 3 is 33. When there will remain 96 tasks (asuming that no other tasks are scheduled meanwhile),
						an other node will be removed at the next calculation time, and so on and so forth... </para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">releaseDelay</emphasis> - Delay between each node release. This time is useful since the deploying time is important.
						Let us assume that a node has to be removed. If this releaseDelay did not exist (or if it was set to 0),
						this node would be removed instantaneously. Let us assume assume that right after this
						removal, another task is scheduled, requiring a new node. In that case, we would lose a lot of time removing the previous node and deploying another one
						whereas the task could have been scheduled on the same node. This releaseDelay therefore represents the time to wait before effectively removing a node.</para>
					</listitem>
				</itemizedlist>
				<note>
					<para>
						This policy is available only when using the <emphasis>ProActive Scheduler</emphasis>.
					</para>
				</note>
		</section>
		<section xml:id="ec2_policy"><info><title>Amazon EC2 policy</title></info>
				<para>
					Allocates resources according to the Scheduler loading factor,
					releases resources considering that EC2 instances are paid by the hour.
				</para>
				<para>For using this policy, you have to precise the following parameters:</para>
				<itemizedlist>
					<listitem>
						<para><emphasis role="bold">nodesAvailableTo</emphasis> - utilization permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">administrator</emphasis> - Administration permission. It should take one of the following values:
							<itemizedlist>
								<listitem>
									<para><emphasis role="bold">"USER"</emphasis> - Only the node source creator</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"GROUP"</emphasis> - Only the group in which the node source creator belongs to</para>
								</listitem>
								<listitem>
									<para><emphasis role="bold">"ALL"</emphasis> - Everybody</para>
								</listitem>
							</itemizedlist>
						</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">schedulerUrl</emphasis> - Url of the Scheduler</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">schedulerCredentialsPath</emphasis> - Path to the credentials used for scheduler authentification.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">preemptive</emphasis> - Preemptive parameter indicates the way of releasing nodes. If it is true, nodes will be released
						without waiting the end of jobs running on (default is false).</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">refreshTime</emphasis> - time between each calculation of the number of needed nodes.</para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">loadFactor</emphasis> - number of tasks per node. Actually, if this number is <emphasis role="italics">N</emphasis>,
						it does not means that there will be exactly <emphasis role="italics">N</emphasis> tasks
						executed on each node. This factor is just used to compute the total number of nodes. For instance, let us assume that this factor is 3 and that we schedule 100
						tasks. In that case, we will have 34 (= upper bound of 100/3) started nodes. Once one task finished and the refresh time passed,
						one node will be removed since 99 divided by 3 is 33. When there will remain 96 tasks (asuming that no other tasks are scheduled meanwhile),
						an other node will be removed at the next calculation time, and so on and so forth... </para>
					</listitem>
					<listitem>
						<para><emphasis role="bold">releaseDelay</emphasis> - Delay between each node release. This time is useful since the deploying time is important.
						Let us assume that a node has to be removed. If this releaseDelay did not exist (or if it was set to 0),
						this node would be removed instantaneously. Let us assume assume that right after this
						removal, another task is scheduled, requiring a new node. In that case, we would lose a lot of time removing the previous node and deploying another one
						whereas the task could have been scheduled on the same node. This releaseDelay therefore represents the time to wait before effectively removing a node.</para>
					</listitem>
				</itemizedlist>
				<note>
					<para>
						This policy is available only when using the <emphasis>ProActive Scheduler</emphasis>.
					</para>
				</note>
		</section>
</section>
